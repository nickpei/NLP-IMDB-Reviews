{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "import nltk\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier as SGD\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations and stopwords in documents\n",
    "def ReadOneFile(fileName):\n",
    "    contents = []\n",
    "    with open(fileName, 'r', encoding='UTF-8') as file:\n",
    "        for line in file:\n",
    "            contents.append(line.rstrip('\\n').lower())\n",
    "    result = ''.join(contents)\n",
    "\n",
    "    #remove punctuations\n",
    "    special_char = [\"‘\", \"’\", \"·\", \"–\", \"“\", \"”\"]\n",
    "    result = result.translate(str.maketrans('', '', string.punctuation)).translate({ord(c): 'special char' for c in special_char})\n",
    "    result = result.split()\n",
    "\n",
    "    #remove stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    result = [w for w in result if w not in stop_words]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Processing documents to save in format as [{word_1: count, word_2: count, ... _FileID_: DocID, _CLASS_: 0 or 1},...]\n",
    "def ReadFiles(fileName):\n",
    "    data = []\n",
    "    directory_top = \"C:/Users/nicho/Desktop/IMDB_Dataset/\" + fileName + \"/\"\n",
    "    for data_class in os.listdir(directory_top):\n",
    "        directory_class = directory_top + data_class + \"/\"\n",
    "        for file in os.listdir(directory_class):\n",
    "            words = ReadOneFile(directory_class + file)\n",
    "            example = {x:words.count(x) for x in words}\n",
    "            example['__FileID__'] = file\n",
    "            example['__CLASS__'] = 1 if data_class[:3] == 'pos' else 0\n",
    "            data.append(example)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = ReadFiles(\"train\")\n",
    "# data_test = ReadFiles(\"test\")\n",
    "df = pd.DataFrame(data_train).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the processing ran pretty slow inside Jupyter, we did the processing in Python IDE and saved the results to local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results\n",
    "with open(r'C:\\Users\\nicho\\Desktop\\data_train_wostopNpunc.txt', 'r', encoding='UTF-8') as f:\n",
    "    data_train = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing all the documents in train folders will result in an 25000 x 121224 array. However, this size exceeds the maximum size that can be allocated. Therefore, we will choose only 2000 documents here instead of all doucments. 1000 from neg and 1000 from pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_1000 = data_train[:1000]\n",
    "last_1000 = data_train[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 28391)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(first_1000 + last_1000).fillna(0)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>man</th>\n",
       "      <th>unnatural</th>\n",
       "      <th>feelings</th>\n",
       "      <th>pig</th>\n",
       "      <th>starts</th>\n",
       "      <th>opening</th>\n",
       "      <th>scene</th>\n",
       "      <th>terrific</th>\n",
       "      <th>example</th>\n",
       "      <th>...</th>\n",
       "      <th>barkers</th>\n",
       "      <th>verges</th>\n",
       "      <th>rosyhued</th>\n",
       "      <th>bluecollar</th>\n",
       "      <th>overtures</th>\n",
       "      <th>illiteracy</th>\n",
       "      <th>plottool</th>\n",
       "      <th>colorless</th>\n",
       "      <th>fluffand</th>\n",
       "      <th>swallowthough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28391 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   story  man  unnatural  feelings  pig  starts  opening  scene  terrific  \\\n",
       "0    1.0  1.0        1.0       1.0  1.0     1.0      1.0    1.0       1.0   \n",
       "1    0.0  0.0        0.0       0.0  0.0     1.0      1.0    0.0       0.0   \n",
       "2    0.0  1.0        0.0       0.0  0.0     0.0      0.0    0.0       0.0   \n",
       "3    1.0  0.0        0.0       0.0  0.0     0.0      0.0    2.0       0.0   \n",
       "4    1.0  0.0        0.0       0.0  0.0     0.0      0.0    0.0       0.0   \n",
       "\n",
       "   example  ...  barkers  verges  rosyhued  bluecollar  overtures  illiteracy  \\\n",
       "0      1.0  ...      0.0     0.0       0.0         0.0        0.0         0.0   \n",
       "1      0.0  ...      0.0     0.0       0.0         0.0        0.0         0.0   \n",
       "2      0.0  ...      0.0     0.0       0.0         0.0        0.0         0.0   \n",
       "3      0.0  ...      0.0     0.0       0.0         0.0        0.0         0.0   \n",
       "4      0.0  ...      0.0     0.0       0.0         0.0        0.0         0.0   \n",
       "\n",
       "   plottool  colorless  fluffand  swallowthough  \n",
       "0       0.0        0.0       0.0            0.0  \n",
       "1       0.0        0.0       0.0            0.0  \n",
       "2       0.0        0.0       0.0            0.0  \n",
       "3       0.0        0.0       0.0            0.0  \n",
       "4       0.0        0.0       0.0            0.0  \n",
       "\n",
       "[5 rows x 28391 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will do the test only on these 2000 docuement to see the performance. Therefore, we will split the data into training set and validation set. Later on, we will do the testing with test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(['__FileID__', '__CLASS__'], axis=1)\n",
    "labels = df.__CLASS__\n",
    "X_train, X_val, Y_train, Y_val = sklearn.model_selection.train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 28389) (400, 28389) (1600,) (400,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, Y_train.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Model Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Note: Because the models are running pretty slow in Juptyer, we ran all the models locally and saved the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 1.0 \n",
      "Validation accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "LogisticReg = sklearn.linear_model.LogisticRegression(solver='lbfgs')\n",
    "LogisticReg.fit(X_train, Y_train)\n",
    "print(\"Training accuracy:\", LogisticReg.score(X_train, Y_train), \"\\nValidation accuracy:\", \n",
    "      LogisticReg.score(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signle Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTree = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "DecisionTree.fit(X_train, Y_train)\n",
    "print(\"Training acc:\", DecisionTree.score(X_train, Y_train), \"\\nValidation acc:\", \n",
    "      DecisionTree.score(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Training acc: 1.0  \n",
    "    Validation acc: 0.685"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"max_depth\": [None, 10, 100, 1000],\n",
    "              \"min_samples_split\": [5, 10, 50, 100, 500, 1000],\n",
    "              \"min_samples_leaf\": [10, 100, 1000],\n",
    "              \"max_leaf_nodes\": [None, 10, 100, 1000],\n",
    "              }\n",
    "dt_search = GridSearchCV(DecisionTree, parameters)\n",
    "dt_search.fit(X_train, Y_train)\n",
    "print(\"The best parameters: \" + str(dt_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters: {'max_depth': None, 'max_leaf_nodes': 1000, 'min_samples_leaf': 10, 'min_samples_split': 50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTree2 = tree.DecisionTreeClassifier(criterion = \"entropy\", max_depth = None, max_leaf_nodes = 1000, \n",
    "                                            min_samples_leaf = 10, min_samples_split = 50)\n",
    "DecisionTree2.fit(X_train, Y_train)\n",
    "print(\"Training acc:\", DecisionTree2.score(X_train, Y_train), \"\\nValidation acc:\", \n",
    "      DecisionTree2.score(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Training acc: 0.82 \n",
    "    Validation acc: 0.6625"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Boost = AdaBoostClassifier(base_estimator=DecisionTree2, n_estimators=100)\n",
    "Boost.fit(X_train, Y_train)\n",
    "print(\"Training acc:\", Boost.score(X_train, Y_train), \"\\nValidation acc:\",\n",
    "      Boost.score(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Training acc: 1.0 \n",
    "    Validation acc: 0.845"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest = RandomForestClassifier(criterion = 'entropy', n_estimators=100)\n",
    "RandomForest.fit(X_train, Y_train)\n",
    "print(\"Training acc:\", RandomForest.score(X_train, Y_train), \"\\nValidation acc:\",\n",
    "      RandomForest.score(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Training acc: 1.0 \n",
    "    Validation acc: 0.855"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"min_samples_split\": [2, 5, 10, 20],\n",
    "              \"max_depth\": [None, 2, 5, 10, 20],\n",
    "              \"min_samples_leaf\": [1, 5, 10, 20],\n",
    "              \"max_leaf_nodes\": [None, 5, 10, 20, 50],\n",
    "              }\n",
    "rfc_search = GridSearchCV(RandomForest, parameters)\n",
    "rfc_search.fit(X_train, Y_train)\n",
    "print(\"The best parameters: \" + str(rfc_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The best parameters: {'max_depth': None, 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForest2 = RandomForestClassifier(criterion = \"entropy\", max_depth = None, max_leaf_nodes = None, \n",
    "                                            min_samples_leaf = 1, min_samples_split = 5)\n",
    "RandomForest2.fit(X_train, Y_train)\n",
    "print(\"Training acc:\", RandomForest2.score(X_train, Y_train), \"\\nValidation acc:\", \n",
    "      RandomForest2.score(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Training acc: 1.0 \n",
    "    Validation acc: 0.88"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Boost = AdaBoostClassifier(base_estimator=RandomForest2, n_estimators=100)\n",
    "Boost.fit(X_train, Y_train)\n",
    "print(\"Training acc:\", Boost.score(X_train, Y_train), \"\\nValidation acc:\",\n",
    "      Boost.score(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Training acc: 1.0 \n",
    "    Validation acc: 0.8725"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = SVC(probability=True)\n",
    "SVM.fit(X_train, Y_train)\n",
    "print(\"Training acc:\", SVM.score(X_train, Y_train), \"\\nValidation acc:\", \n",
    "      SVM.score(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Training acc: 0.988125 \n",
    "    Validation acc: 0.8775"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'kernel': ['rbf'], 'gamma': [0.01, 0.005, 0.001], 'C': [0.5, 1, 1.5, 2, 4]},\n",
    "              {'kernel': ['linear'], 'C': [0.001, 0.01, 0.1, 1]}]\n",
    "svm_search = GridSearchCV(SVM, parameters, cv=5, scoring=\"roc_auc\", n_jobs=4)\n",
    "svm_search.fit(X_train, Y_train)\n",
    "print(\"The best parameters: \" + str(svm_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The best parameters: {'C': 4, 'gamma': 0.001, 'kernel': 'rbf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM2 = SVC(probability=True, kernel='rbf', C=4 ,gamma=0.001)\n",
    "SVM2.fit(X_train, Y_train)\n",
    "print(\"Training acc:\", SVM2.score(X_train, Y_train), \"\\nValidation acc:\", \n",
    "      SVM2.score(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Training acc: 0.985625 \n",
    "    Validation acc: 0.8975"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaiveBayes = MNB()\n",
    "NaiveBayes.fit(X_train, Y_train)\n",
    "print(\"Training acc:\", NaiveBayes.score(X_train, Y_train), \"\\nValidation acc:\",\n",
    "      NaiveBayes.score(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Training acc: 0.99 \n",
    "    Validation acc: 0.9325"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(max_iter=5, random_state=0,loss='modified_huber',n_jobs=4)\n",
    "sgd.fit(X_train, Y_train)\n",
    "print(\"Training acc:\", sgd.score(X_train, Y_train), \"\\nValidation acc:\",\n",
    "      sgd.score(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Training acc: 0.995 \n",
    "    Validation acc: 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'alpha': [0.1, 0.5, 1, 1.5]}\n",
    "sgd_search = GridSearchCV(sgd,parameters , scoring='roc_auc', cv=20)  \n",
    "sgd_search.fit(X_train, Y_train)\n",
    "print(\"The best parameters: \" + str(sgd_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The best parameters: {'alpha': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd2 = SGD(max_iter=5, random_state=0,loss='modified_huber',n_jobs=4,alpha=0.1)\n",
    "sgd2.fit(X_train, Y_train)\n",
    "print(\"Training acc:\", sgd2.score(X_train, Y_train), \"\\nValidation acc:\", \n",
    "      sgd2.score(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Training acc: 0.99875 \n",
    "    Validation acc: 0.9025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
